package org.aksw.challenge;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileOutputStream;
import java.io.InputStreamReader;
import java.util.HashMap;
import java.util.Map;

import javax.sql.DataSource;

import org.aksw.commons.util.compress.MetaBZip2CompressorInputStream;
import org.aksw.commons.util.strings.StringUtils;
import org.aksw.jena_sparql_api.cache.extra.CacheBackend;
import org.aksw.jena_sparql_api.cache.extra.CacheFrontend;
import org.aksw.jena_sparql_api.cache.extra.CacheFrontendImpl;
import org.aksw.jena_sparql_api.cache.staging.CacheBackendDaoPostgres;
import org.aksw.jena_sparql_api.cache.staging.CacheBackendDataSource;
import org.aksw.jena_sparql_api.core.FluentQueryExecutionFactory;
import org.aksw.jena_sparql_api.core.QueryExecutionFactory;
import org.apache.commons.compress.compressors.bzip2.BZip2CompressorOutputStream;
import org.apache.jena.atlas.web.HttpException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.core.io.ClassPathResource;
import org.springframework.core.io.Resource;
import org.springframework.jdbc.datasource.DriverManagerDataSource;

import com.hp.hpl.jena.graph.Node;
import com.hp.hpl.jena.query.Query;
import com.hp.hpl.jena.query.QueryExecution;
import com.hp.hpl.jena.query.QueryFactory;
import com.hp.hpl.jena.query.ResultSet;
import com.hp.hpl.jena.query.ResultSetFormatter;
import com.hp.hpl.jena.sparql.core.Prologue;
import com.jolbox.bonecp.BoneCPConfig;
import com.jolbox.bonecp.BoneCPDataSource;

public class MainAkswChallenge2015 {

    private static final Logger logger = LoggerFactory.getLogger(MainAkswChallenge2015.class);

    public static void main(String[] args) throws Exception {
        Resource queryLog = new ClassPathResource("trained_queries.txt.bz2");
        MetaBZip2CompressorInputStream in = new MetaBZip2CompressorInputStream(queryLog.getInputStream());

        BufferedReader reader = new BufferedReader(new InputStreamReader(in));

        DriverManagerDataSource coreDs = new DriverManagerDataSource();
        coreDs.setDriverClassName("org.postgresql.Driver");
        coreDs.setUrl("jdbc:postgresql://localhost:5432/facete2tomcatcommon");
        coreDs.setUsername("postgres");
        coreDs.setPassword("postgres");


        DataSource ds;

        boolean useBoneCpWrapper = true;
        if(useBoneCpWrapper) {

            BoneCPConfig cpConfig = new BoneCPConfig();
            cpConfig.setDatasourceBean(coreDs);

            cpConfig.setMinConnectionsPerPartition(1);
            cpConfig.setMaxConnectionsPerPartition(10);
            cpConfig.setPartitionCount(2);
            cpConfig.setCloseConnectionWatch(true);

            try {
                ds = new BoneCPDataSource(cpConfig);
            } catch(Exception e) {
                throw new RuntimeException(e);
            }
        } else {
            ds = coreDs;
        }


//        PG
//
        CacheBackend cacheBackend = new CacheBackendDataSource(ds, new CacheBackendDaoPostgres(1000000l));
        CacheFrontend cacheFrontend = new CacheFrontendImpl(cacheBackend);

        QueryExecutionFactory qef = FluentQueryExecutionFactory
            .http("http://localhost:8890/sparql", "http://data.semanticweb.org/")
            .config()
                .withCache(cacheFrontend)
            .end()
            .create();

        Prologue prologue = new Prologue();
        //SparqlQueryParser parser = SparqlQueryParserImpl.create(Syntax.syntaxARQ, prologue);


        int parseFailCount = 0;
        int totalQueryCount = 0;
        int emptyResultCount = 0;


        String rawLine;

        Map<Node, Integer> map = new HashMap<Node, Integer>();

        while((rawLine = reader.readLine()) != null) {
            System.out.println("Processing Query #" + totalQueryCount);

            String queryStr = StringUtils.urlDecode(rawLine);

            Query query;
            try {
                query = QueryFactory.create(queryStr);
                //query = parser.apply(queryStr);
            } catch(Exception e) {
                ++parseFailCount;
                continue;
            }

            File dir = new File("cache");
            dir.mkdir();

            String hash = StringUtils.md5Hash("" + query);
            File file = new File(dir + "/" + hash + "sparql-results.xml.bz2");


            QueryExecution qe = qef.createQueryExecution(query);
            if(query.isConstructType()) {
                throw new RuntimeException("should not happen");
            } else if(query.isAskType()) {

            } else if(query.isSelectType()) {
                try {
                    ResultSet rs = qe.execSelect();
                    FileOutputStream tmp = new FileOutputStream(file);
                    BZip2CompressorOutputStream out = new BZip2CompressorOutputStream(tmp);

                    ResultSetFormatter.outputAsXML(out, rs);
                    out.flush();
                    out.close();
                } catch(Exception e) {
                    if(e instanceof HttpException) {
                        HttpException x = (HttpException)e;
                        logger.warn("Error on " + query);
                        logger.warn(x.getResponse());
                    }
                }
            } else if(query.isDescribeType()) {

            }
            qe.abort();


            ++totalQueryCount;
        }

        logger.info("total: " + totalQueryCount);
        logger.info("parseFailCount: " + parseFailCount);
        logger.info("empty result: " + emptyResultCount);
    }
}
